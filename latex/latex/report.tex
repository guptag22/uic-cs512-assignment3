\documentclass[11pt]{report}
\usepackage{./assignmentMod}
\usepackage{slashbox}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{enumerate}
\usepackage[shortlabels]{enumitem}
\usepackage{stmaryrd}
\usepackage[final]{pdfpages}
\usepackage{array}
\usepackage{multirow}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{authblk}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{epstopdf}

\input{./Definitions}

\begin{document}
\title{
  CS512: Advanced Machine Learning. \\
  \large Assignment 3: Adversarial Training on Sequence Classification}

\author{Garima Gupta: ggupta22@uic.edu \and Sai Teja Karnati: skarna3@uic.edu \and
 Shubham Singh: ssing57@uic.edu \and Wangfei Wang: wwang75@uic.edu}

\graphicspath{{../../Figures/}}

\maketitle

\section{Introduction}
\section{(15 points) Training the Basic Model}

Hyperparameters values: 

\texttt{batch\_size = 27}, \texttt{hidden\_size = 10}, \texttt{basic\_epoch = 100}, \texttt{out\_channels = 64}, \texttt{kernel\_size = 10},  \texttt{stride = 3}, \texttt{lr = 1e-3} (learning rate), \texttt{weight\_decay = 1e-3}. 

\begin{figure}[h]
    \includegraphics[width = 15 cm]{BasicModel.png}
    \centering
\end{figure}




\section{(10 points)  Save and Load Pretrained Model}

See code in \texttt{training.py}. In the code, we commented this part as \texttt{Part 3, Save and Load model}.  

\section{(25 points) Adversarial Training as Regularization}
\begin{itemize}
    \item[a] \textbf{(10 points)} See the \texttt{compute\_perturbation} function in \texttt{training.py}.
    \item[b] \textbf{(5 points)} See the branch \texttt{mode} = `AdvLSTM' in \texttt{LSTMClassifier} in \texttt{Classifier.py}.
    \item[c] \textbf{(10 points)} 

    Among the $\epsilon$'s we have tried ($\epsilon = [0, 2, 4, 6, 8, 10, 0.001, 0.01, 0.1, 1, 10, 100, 1000]$), $\epsilon = 6$ gives the optimal performance at the end of 100 epochs. The other hyperparameters were set the same as those in the basic model. 

    As shown in the figure above, the performance of the model changes slightly with the change of $\epsilon$, meaning our model is pretty robust to disturbance. 
    At the end of epoch 50, $\epsilon = 0.01$ seems to give the best test accuracy among $\epsilon = [0.01, 0.1, 1]$. 
    But again, the test accuracies are pretty similar in the set of $\epsilon$'s we have tried.    

    \begin{figure}[h]
    	\includegraphics[width = 15 cm]{AdvModel.png}
    	\centering
	\end{figure}

\end{itemize}

	


\section{(40 points) Adversarial Training as Proximal Mapping}

\section{(10 points) Dropout and Batch Normalization}





\end{document}
